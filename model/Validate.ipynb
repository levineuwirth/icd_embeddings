{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705bf087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 20:47:44.643634: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 20:47:45.005701: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-04 20:47:46.158362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-04 20:47:46.158416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-04 20:47:46.411411: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-04 20:47:46.949314: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-04 20:47:46.953786: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-04 20:47:51.178019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from keras.saving import load_model\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6fe6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "def f2_score(y_true, y_pred):\n",
    "    # Ensure inputs are tensors\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    tp = tf.reduce_sum(y_true * y_pred)\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "    # Calculate precision and recall\n",
    "    epsilon = tf.keras.backend.epsilon()  # Small constant to avoid division by zero\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "    # Calculate F2 score\n",
    "    f2 = (5 * precision * recall) / (4 * precision + recall + epsilon)\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f720e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten, concatenate, MultiHeadAttention, LayerNormalization, Add\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb25a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class DeepSet(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_encode, num_decode, **kwargs):\n",
    "        super(DeepSet, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_encode = num_encode\n",
    "        self.num_decode = num_decode\n",
    "        # # Element-wise transformation: phi network\n",
    "        # self.phi = tf.keras.Sequential([\n",
    "        #     Dense(self.hidden_dim, activation='relu')\n",
    "        # ])\n",
    "        # # Post-aggregation transformation: rho network\n",
    "        # self.rho = tf.keras.Sequential([\n",
    "        #     Dense(self.output_dim, activation='relu')\n",
    "        # ])\n",
    "\n",
    "        # Element-wise transformation: phi network\n",
    "        self.phi = tf.keras.Sequential([\n",
    "            Dense(self.hidden_dim, activation='relu') for _ in range(self.num_encode)\n",
    "        ])\n",
    "\n",
    "        # Post-aggregation transformation: rho network\n",
    "        self.rho = tf.keras.Sequential([\n",
    "            Dense(self.hidden_dim, activation='relu') for _ in range(self.num_decode - 1)\n",
    "        ] + [Dense(self.output_dim, activation='relu')])  # Last layer should output correct dimension\n",
    "\n",
    "    def call(self, x):\n",
    "        transformed = self.phi(x)  # (batch_size, num_codes, hidden_dim)\n",
    "        aggregated = tf.reduce_sum(transformed, axis=1)  # (batch_size, hidden_dim)\n",
    "        output = self.rho(aggregated)  # (batch_size, output_dim)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DeepSet, self).get_config()\n",
    "        config.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"hidden_dim\": self.hidden_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"num_encode\": self.num_encode,\n",
    "            \"num_decode\": self.num_decode\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31346189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten, concatenate, MultiHeadAttention, LayerNormalization, Add\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import AUC, Precision, Recall\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"ff_dim\": self.ff_dim,\n",
    "            \"rate\": self.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9d15bc-24d5-43ee-88ff-1bf365587e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b3268dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_file_path = 'data/NRD_2019_Small.dta'\n",
    "# Load new data\n",
    "new_data = pd.read_stata(new_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03cf55ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'died', 'i10_dx1', 'i10_dx2', 'i10_dx3', 'i10_dx4', 'i10_dx5', 'i10_dx6', 'i10_dx7', 'i10_dx8', 'i10_dx9', 'i10_dx10', 'i10_dx11', 'i10_dx12', 'i10_dx13', 'i10_dx14', 'i10_dx15', 'i10_dx16', 'i10_dx17', 'i10_dx18', 'i10_dx19', 'i10_dx20', 'i10_dx21', 'i10_dx22', 'i10_dx23', 'i10_dx24', 'i10_dx25', 'i10_dx26', 'i10_dx27', 'i10_dx28', 'i10_dx29', 'i10_dx30', 'i10_dx31', 'i10_dx32', 'i10_dx33', 'i10_dx34', 'i10_dx35', 'i10_dx36', 'i10_dx37', 'i10_dx38', 'i10_dx39', 'i10_dx40', 'elective', 'female', 'key_nrd', 'i10_birth', 'i10_delivery', 'i10_injury', 'i10_multinjury', 'i10_serviceline', 'pay1', 'pclass_orproc', 'i10_pr16', 'i10_pr17', 'i10_pr18', 'i10_pr19', 'i10_pr20', 'i10_pr21', 'i10_pr22', 'i10_pr23', 'i10_pr24', 'i10_pr25', 'prday16', 'prday17', 'prday18', 'prday19', 'prday20', 'prday21', 'prday22', 'prday23', 'prday24', 'prday25', 'year', 'zipinc_qrtl', 'missing', 'mor30', 'rea30', 'charlindex', 'age_adjust', 'charlindex_age_adjust', 'Index_Readmission', 'Index_Mortality', '_merge']\n"
     ]
    }
   ],
   "source": [
    "column_names = new_data.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53cce7c-905d-42e9-864a-9ba46d6bb8e7",
   "metadata": {
    "id": "b53cce7c-905d-42e9-864a-9ba46d6bb8e7",
    "outputId": "2999ac1a-4458-4b0c-8b2b-7e21178c61db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 20:49:07.665065: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 51 variables whereas the saved optimizer has 1 variables. \n",
      "/users/xwang259/.conda/envs/tf215_env/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/users/xwang259/.conda/envs/tf215_env/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "Encoding ICD codes: 100%|██████████| 35/35 [00:00<00:00, 47.28it/s]\n",
      "Making Predictions: 100%|██████████| 48/48 [00:07<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7343\n",
      "95% CI for AUC: [0.7274, 0.7403]\n",
      "Accuracy: 0.4252\n",
      "Precision: 0.1659\n",
      "Recall: 0.9191\n",
      "F1 Score: 0.2811\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the outcome variable and file path\n",
    "outcome_var = 'REA30'  # Set outcome variable here, e.g., 'DIED', 'MOR30', 'REA30'\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('Model/readmit_hypertrial_deepset.keras')\n",
    "\n",
    "# Load the LabelEncoder for ICD codes\n",
    "with open('./Model/readmit_2016_label_encoder.pkl', 'rb') as file:\n",
    "    encoder = pickle.load(file)\n",
    "\n",
    "# Load the MinMaxScaler for 'AGE'\n",
    "with open('./Model/readmit_2016_age_scaler.pkl', 'rb') as file:\n",
    "    age_scaler = pickle.load(file)\n",
    "\n",
    "# Convert all column names to uppercase\n",
    "new_data.columns = new_data.columns.str.upper()\n",
    "\n",
    "# Filter out observations where DIED == 1 if outcome_var is REA30\n",
    "if outcome_var == 'REA30' and 'DIED' in new_data.columns:\n",
    "    new_data = new_data[new_data['DIED'] != 1]\n",
    "\n",
    "# Rename columns to match the training data\n",
    "column_mapping = {\n",
    "    'AGE': 'AGE',\n",
    "    'FEMALE': 'FEMALE',\n",
    "    outcome_var: outcome_var,  # Rename outcome variable\n",
    "}\n",
    "icd_column_mapping = {f'I10_DX{i}': f'I10_DX{i}' for i in range(1, 31)}\n",
    "column_mapping.update(icd_column_mapping)\n",
    "new_data.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Define ICD columns\n",
    "icd_columns = [f'I10_DX{i}' for i in range(1, 36)]\n",
    "\n",
    "# Encode ICD codes\n",
    "label_to_int = {label: idx for idx, label in enumerate(encoder.classes_)}\n",
    "unknown_label_int = 0  # Assign unknown codes to index 0\n",
    "\n",
    "for col in tqdm(icd_columns, desc=\"Encoding ICD codes\"):\n",
    "    new_data[col] = new_data[col].astype(str).str.upper()  # Convert to uppercase\n",
    "    new_data[col] = new_data[col].map(label_to_int).fillna(unknown_label_int).astype(int)\n",
    "\n",
    "# Normalize 'AGE'\n",
    "new_data['AGE'] = age_scaler.transform(new_data[['AGE']])\n",
    "\n",
    "# One-hot encode 'PAY1' and 'ZIPINC_QRTL' only (excluding 'RACE')\n",
    "new_data = pd.get_dummies(new_data, columns=['PAY1', 'ZIPINC_QRTL'], prefix=['PAY1', 'ZIPINC_QRTL'])\n",
    "\n",
    "# Ensure that all expected one-hot encoded columns are present\n",
    "def ensure_columns(data, expected_columns):\n",
    "    for col in expected_columns:\n",
    "        if col not in data.columns:\n",
    "            data[col] = 0\n",
    "    return data\n",
    "\n",
    "# Define one-hot encoded columns for 'PAY1' and 'ZIPINC_QRTL' based on training data\n",
    "pay1_columns = [col for col in new_data.columns if col.startswith('PAY1_')]\n",
    "zipinc_qrtl_columns = [col for col in new_data.columns if col.startswith('ZIPINC_QRTL_')]\n",
    "\n",
    "# Ensure all expected columns are present\n",
    "new_data = ensure_columns(new_data, pay1_columns + zipinc_qrtl_columns)\n",
    "\n",
    "# Prepare input features\n",
    "X_new = new_data[['AGE', 'FEMALE'] + pay1_columns + zipinc_qrtl_columns + icd_columns]\n",
    "\n",
    "# Drop rows with missing values\n",
    "X_new = X_new.dropna()\n",
    "if outcome_var in new_data.columns:\n",
    "    y_new = new_data.loc[X_new.index, outcome_var].dropna()\n",
    "    X_new = X_new.loc[y_new.index]\n",
    "else:\n",
    "    y_new = None\n",
    "\n",
    "# Convert input features to the required data types\n",
    "X_new = X_new.astype('float32')\n",
    "\n",
    "# Make predictions\n",
    "batch_size = 1024\n",
    "num_samples = len(X_new)\n",
    "y_pred_prob = []\n",
    "\n",
    "# Prepare the inputs in the same structure as during training\n",
    "for start_idx in tqdm(range(0, num_samples, batch_size), desc=\"Making Predictions\"):\n",
    "    end_idx = min(start_idx + batch_size, num_samples)\n",
    "    batch_inputs = [\n",
    "        X_new[icd_columns].iloc[start_idx:end_idx],\n",
    "        X_new['AGE'].iloc[start_idx:end_idx].values,\n",
    "        X_new['FEMALE'].iloc[start_idx:end_idx].values,\n",
    "    ] + [X_new[col].iloc[start_idx:end_idx].values for col in pay1_columns] \\\n",
    "      + [X_new[col].iloc[start_idx:end_idx].values for col in zipinc_qrtl_columns]\n",
    "\n",
    "    batch_pred_prob = model.predict(batch_inputs, verbose=0)\n",
    "    y_pred_prob.extend(batch_pred_prob.flatten())\n",
    "\n",
    "y_pred_prob = np.array(y_pred_prob)\n",
    "\n",
    "# Function to calculate the 95% CI for AUC using bootstrapping\n",
    "def calculate_auc_ci(y_true, y_pred_prob, n_bootstraps=1000, ci=0.95):\n",
    "    bootstrapped_aucs = []\n",
    "    np.random.seed(42)\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Sample with replacement from the data\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # Skip resamples that do not have both classes present\n",
    "            continue\n",
    "\n",
    "        score = roc_auc_score(y_true[indices], y_pred_prob[indices])\n",
    "        bootstrapped_aucs.append(score)\n",
    "\n",
    "    # Calculate CI\n",
    "    sorted_aucs = np.sort(bootstrapped_aucs)\n",
    "    lower_bound = sorted_aucs[int((1 - ci) / 2 * len(sorted_aucs))]\n",
    "    upper_bound = sorted_aucs[int((1 + ci) / 2 * len(sorted_aucs))]\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Evaluate the model if true labels are available\n",
    "if y_new is not None:\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    if len(np.unique(y_new)) > 1:\n",
    "        auc = roc_auc_score(y_new, y_pred_prob)\n",
    "        accuracy = accuracy_score(y_new, y_pred)\n",
    "        precision = precision_score(y_new, y_pred)\n",
    "        recall = recall_score(y_new, y_pred)\n",
    "        f1 = f1_score(y_new, y_pred)\n",
    "\n",
    "        # Calculate 95% CI for AUC\n",
    "        auc_lower, auc_upper = calculate_auc_ci(y_new.values, y_pred_prob)\n",
    "\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "        print(f\"95% CI for AUC: [{auc_lower:.4f}, {auc_upper:.4f}]\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "    else:\n",
    "        accuracy = accuracy_score(y_new, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"Predictions:\")\n",
    "    print(y_pred_prob)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "new_data_filtered = new_data.loc[X_new.index].reset_index(drop=True)\n",
    "new_data_filtered['Predicted_Probability'] = y_pred_prob\n",
    "if y_new is not None:\n",
    "    new_data_filtered['Predicted_Label'] = y_pred\n",
    "new_data_filtered.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eeb95df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score: 0.4817\n"
     ]
    }
   ],
   "source": [
    "f2 = f2_score(y_new, y_pred_prob)\n",
    "print(f\"F2 Score: {f2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea22dccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22152314 0.12376571 0.34946087 ... 0.7253336  0.36996785 0.7213385 ]\n",
      "0.0\n",
      "0.28109879419369704\n"
     ]
    }
   ],
   "source": [
    "optimal_threshold = 0\n",
    "best_f1 = 0\n",
    "print(y_pred_prob)\n",
    "for threshold in np.linspace(0, 1, 100):\n",
    "    preds = (y_pred_prob > threshold).astype(int)\n",
    "    f1 = f1_score(y_new, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        optimal_threshold = threshold\n",
    "print(optimal_threshold)\n",
    "print(best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5cc106-202a-4075-bec9-7bad486bb268",
   "metadata": {
    "id": "1d5cc106-202a-4075-bec9-7bad486bb268",
    "outputId": "39824bd4-4937-45b8-f5af-73c94ca0b556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating INDEX_MORTALITY against REA30...\n",
      "Validating INDEX_READMISSION against REA30...\n",
      "Validating CHARLINDEX against REA30...\n",
      "Validating CHARLINDEX_AGE_ADJUST against REA30...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# List of traditional indexes to validate\n",
    "traditional_indexes = ['INDEX_MORTALITY', 'INDEX_READMISSION', 'CHARLINDEX', 'CHARLINDEX_AGE_ADJUST']\n",
    "outcome_var = 'REA30'  # Set outcome variable here, e.g., 'DIED', 'MOR30', 'REA30'\n",
    "new_data_file_path = 'data/NRD_2019_Small.dta'\n",
    "\n",
    "# Load new data\n",
    "validation_data = pd.read_stata(new_data_file_path)\n",
    "# Convert all column names to uppercase\n",
    "validation_data.columns = validation_data.columns.str.upper()\n",
    "\n",
    "# Ensure the dataset contains the required variables\n",
    "missing_columns = [col for col in traditional_indexes + [outcome_var] if col not in validation_data.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following columns are missing in the validation dataset: {missing_columns}\")\n",
    "\n",
    "# Function to calculate 95% CI for AUC using bootstrapping\n",
    "def calculate_auc_ci(y_true, y_pred_prob, n_bootstraps=1000, ci=0.95):\n",
    "    # Convert to NumPy arrays for indexing\n",
    "    y_true = y_true.to_numpy()\n",
    "    y_pred_prob = y_pred_prob.to_numpy()\n",
    "\n",
    "    bootstrapped_aucs = []\n",
    "    np.random.seed(42)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Generate random sample indices\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "\n",
    "        # Ensure both classes are present in the sample\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "\n",
    "        # Calculate AUC for the bootstrap sample\n",
    "        score = roc_auc_score(y_true[indices], y_pred_prob[indices])\n",
    "        bootstrapped_aucs.append(score)\n",
    "\n",
    "    # Calculate CI bounds\n",
    "    lower_bound = np.percentile(bootstrapped_aucs, (1 - ci) / 2 * 100)\n",
    "    upper_bound = np.percentile(bootstrapped_aucs, (1 + ci) / 2 * 100)\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Initialize a results list\n",
    "results = []\n",
    "\n",
    "# Validate each traditional index\n",
    "for index in traditional_indexes:\n",
    "    print(f\"Validating {index} against {outcome_var}...\")\n",
    "\n",
    "    # Filter data to exclude rows with missing values in the index or outcome variable\n",
    "    data = validation_data[[index, outcome_var]].dropna()\n",
    "    y_true = data[outcome_var]\n",
    "    y_pred_prob = data[index]\n",
    "\n",
    "    # Convert index scores to binary predictions (if applicable)\n",
    "    y_pred = (y_pred_prob > 0).astype(int)\n",
    "\n",
    "    # Ensure both classes are present\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        # Calculate evaluation metrics\n",
    "        auc = roc_auc_score(y_true, y_pred_prob)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        f2 = f2_score(y_true, y_pred)\n",
    "\n",
    "        # Calculate AUC confidence intervals\n",
    "        auc_lower, auc_upper = calculate_auc_ci(y_true, y_pred_prob)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Index': index,\n",
    "            'AUC': auc,\n",
    "            'AUC Lower CI': auc_lower,\n",
    "            'AUC Upper CI': auc_upper,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'F2 Score': float(f2)\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Skipped {index}: Only one class present in {outcome_var}\")\n",
    "\n",
    "# Convert results to a DataFrame for easier analysis\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nValidation Metrics for Traditional Scores:\")\n",
    "    print(results_df.to_string(index=False))  # Clean output without indices\n",
    "else:\n",
    "    print(\"No metrics were calculated due to insufficient class diversity in the data.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf215_env",
   "language": "python",
   "name": "tf215_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
